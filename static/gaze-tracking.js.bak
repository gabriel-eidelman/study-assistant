// WebGazer.js Implementation for Study Assistant
// This file should be added to your project and imported in enhanced_client.html

// =========================================================
// 1. INITIALIZATION AND SETUP
// =========================================================

// Global variables for gaze tracking
let gazeTracking = {
    isCalibrated: false,
    isTracking: false,
    calibrationPoints: 0,
    requiredCalibrationPoints: 5,
    gazeData: [],
    gazeHistory: [], // Last 30 gaze points
    lookingAtScreen: true,
    lookingAtContent: true,
    blinkDetected: false,
    lastBlinkTime: Date.now(),
    blinkCount: 0,
    blinkHistory: [], // For blink rate calculation
    attentionScore: 1.0,
    inScreenPercentage: 100,
    inContentPercentage: 100,
    contentArea: { // Will be set based on actual content area
      left: 0,
      top: 0,
      right: 0,
      bottom: 0
    },
    gazeDataValid: false,
    predictionError: 0
  };
  
  /**
   * Initialize WebGazer and prepare for calibration
   * Should be called when the user starts the webcam
   */
  async function initializeGazeTracking() {
    try {
      // Check if WebGazer is available
      if (typeof webgazer === 'undefined') {
        console.error("WebGazer library not loaded");
        showError("Gaze tracking library not available. Falling back to basic tracking.");
        return false;
      }
      
      // Set up WebGazer with custom options
      webgazer.params.showVideo = true; // Show video during calibration
      webgazer.params.showFaceOverlay = true; // Show face tracking
      webgazer.params.showFaceFeedbackBox = true; // Show face tracking box
      webgazer.params.showGazeDot = true; // Show gaze dot during calibration
      
      // Reduce storage to avoid memory issues
      webgazer.params.storingPoints = false;
      
      // Start WebGazer
      await webgazer.begin();
      
      // Set up the gaze listener
      webgazer.setGazeListener(function(data, elapsedTime) {
        if (data == null) {
          // No gaze detected - possible blink or looking away
          handleGazeDataMissing();
        } else {
          // Process valid gaze data
          processGazeData(data);
        }
      });
      
      // Define content areas (where study materials appear)
      updateContentArea();
      
      // Add window resize handler to update content area
      window.addEventListener('resize', updateContentArea);
      
      gazeTracking.isTracking = true;
      return true;
    } catch (error) {
      console.error("Error initializing WebGazer:", error);
      showError(`Gaze tracking initialization failed: ${error.message}. Falling back to basic tracking.`);
      return false;
    }
  }
  
  /**
   * Update the definition of content area (where study materials appear)
   * Should be called on window resize or content changes
   */
  function updateContentArea() {
    // Get the main content element where study material is shown
    // Modify this to target your specific content area
    const videoContainer = document.querySelector('.video-container');
    
    if (videoContainer) {
      const rect = videoContainer.getBoundingClientRect();
      
      // Define content area
      gazeTracking.contentArea = {
        left: rect.left,
        top: rect.top,
        right: rect.right,
        bottom: rect.bottom,
        width: rect.width,
        height: rect.height
      };
    } else {
      // Default to central 70% of screen if element not found
      gazeTracking.contentArea = {
        left: window.innerWidth * 0.15,
        top: window.innerHeight * 0.15,
        right: window.innerWidth * 0.85,
        bottom: window.innerHeight * 0.85,
        width: window.innerWidth * 0.7,
        height: window.innerHeight * 0.7
      };
    }
  }
  
  // =========================================================
  // 2. CALIBRATION SYSTEM
  // =========================================================
  
  /**
   * Start the calibration process
   * This should be integrated with your study session start
   */
  function startCalibration() {
    if (!gazeTracking.isTracking) {
      console.error("Gaze tracking not initialized");
      return false;
    }
    
    // Reset calibration state
    gazeTracking.isCalibrated = false;
    gazeTracking.calibrationPoints = 0;
    
    // Create calibration overlay
    createCalibrationOverlay();
    
    // Show instructions to user
    const calibrationInstructions = document.getElementById('calibration-instructions');
    if (calibrationInstructions) {
      calibrationInstructions.textContent = 
        "Please click on each dot that appears to calibrate eye tracking.";
      calibrationInstructions.style.display = 'block';
    }
    
    // Start calibration sequence
    showNextCalibrationPoint();
    
    return true;
  }
  
  /**
   * Create overlay for calibration points
   */
  function createCalibrationOverlay() {
    // Remove existing overlay if it exists
    const existingOverlay = document.getElementById('calibration-overlay');
    if (existingOverlay) {
      document.body.removeChild(existingOverlay);
    }
    
    // Create new overlay
    const overlay = document.createElement('div');
    overlay.id = 'calibration-overlay';
    overlay.style.position = 'fixed';
    overlay.style.top = '0';
    overlay.style.left = '0';
    overlay.style.width = '100%';
    overlay.style.height = '100%';
    overlay.style.backgroundColor = 'rgba(0,0,0,0.7)';
    overlay.style.zIndex = '1000';
    overlay.style.display = 'flex';
    overlay.style.justifyContent = 'center';
    overlay.style.alignItems = 'center';
    overlay.style.flexDirection = 'column';
    
    // Add instructions element
    const instructions = document.createElement('div');
    instructions.id = 'calibration-instructions';
    instructions.style.color = 'white';
    instructions.style.fontSize = '24px';
    instructions.style.marginBottom = '20px';
    overlay.appendChild(instructions);
    
    // Add point container
    const pointContainer = document.createElement('div');
    pointContainer.id = 'calibration-point-container';
    pointContainer.style.position = 'relative';
    pointContainer.style.width = '100%';
    pointContainer.style.height = '70%';
    overlay.appendChild(pointContainer);
    
    document.body.appendChild(overlay);
  }
  
  /**
   * Show the next calibration point
   */
  function showNextCalibrationPoint() {
    // Calibration positions - 9 points across the screen
    const positions = [
      { x: 0.1, y: 0.1 },   // Top-left
      { x: 0.5, y: 0.1 },   // Top-center
      { x: 0.9, y: 0.1 },   // Top-right
      { x: 0.1, y: 0.5 },   // Middle-left
      { x: 0.5, y: 0.5 },   // Center
      { x: 0.9, y: 0.5 },   // Middle-right
      { x: 0.1, y: 0.9 },   // Bottom-left
      { x: 0.5, y: 0.9 },   // Bottom-center
      { x: 0.9, y: 0.9 }    // Bottom-right
    ];
    
    // Get container and clear previous point
    const container = document.getElementById('calibration-point-container');
    container.innerHTML = '';
    
    // Check if calibration is complete
    if (gazeTracking.calibrationPoints >= gazeTracking.requiredCalibrationPoints) {
      finishCalibration();
      return;
    }
    
    // Create new calibration point
    const point = document.createElement('div');
    point.className = 'calibration-point';
    point.style.position = 'absolute';
    point.style.width = '20px';
    point.style.height = '20px';
    point.style.borderRadius = '50%';
    point.style.backgroundColor = 'red';
    point.style.transform = 'translate(-50%, -50%)';
    
    // Position the point
    const position = positions[gazeTracking.calibrationPoints % positions.length];
    point.style.left = (position.x * 100) + '%';
    point.style.top = (position.y * 100) + '%';
    
    // Make point pulsate
    point.style.animation = 'pulse 1.5s infinite';
    const style = document.createElement('style');
    style.innerHTML = `
      @keyframes pulse {
        0% { transform: translate(-50%, -50%) scale(0.8); }
        50% { transform: translate(-50%, -50%) scale(1.2); }
        100% { transform: translate(-50%, -50%) scale(0.8); }
      }
    `;
    document.head.appendChild(style);
    
    // Add click handler to point
    point.addEventListener('click', function() {
      // Add this point for calibration
      webgazer.recordScreenPosition(
        position.x * window.innerWidth,
        position.y * window.innerHeight,
        'click'
      );
      
      // Update calibration count
      gazeTracking.calibrationPoints++;
      
      // Show calibration progress
      const instructions = document.getElementById('calibration-instructions');
      if (instructions) {
        instructions.textContent = 
          `Calibration Progress: ${gazeTracking.calibrationPoints}/${gazeTracking.requiredCalibrationPoints}`;
      }
      
      // Show next point
      setTimeout(showNextCalibrationPoint, 500);
    });
    
    container.appendChild(point);
  }
  
  /**
   * Finish the calibration process
   */
  function finishCalibration() {
    // Hide the calibration overlay
    const overlay = document.getElementById('calibration-overlay');
    if (overlay) {
      overlay.style.display = 'none';
    }
    
    // Update tracking settings
    webgazer.params.showVideo = false;
    webgazer.params.showFaceOverlay = false;
    webgazer.params.showFaceFeedbackBox = false;
    webgazer.params.showGazeDot = false;
    
    // Mark as calibrated
    gazeTracking.isCalibrated = true;
    
    // Show success message
    showNotification("Eye tracking calibrated successfully!");
    
    // Update UI
    trackingStatus.textContent = 'Gaze Tracking';
    trackingStatus.className = 'tracking-status status-active';
  }
  
  // =========================================================
  // 3. GAZE DATA PROCESSING
  // =========================================================
  
  /**
   * Show a notification to the user
   * @param {string} message - The notification message
   */
  function showNotification(message) {
    // Check if notification element exists
    let notification = document.getElementById('notification');
    
    if (!notification) {
      // Create notification element
      notification = document.createElement('div');
      notification.id = 'notification';
      notification.style.position = 'fixed';
      notification.style.top = '10px';
      notification.style.left = '50%';
      notification.style.transform = 'translateX(-50%)';
      notification.style.backgroundColor = '#4CAF50';
      notification.style.color = 'white';
      notification.style.padding = '10px 20px';
      notification.style.borderRadius = '5px';
      notification.style.zIndex = '2000';
      notification.style.transition = 'opacity 0.5s';
      document.body.appendChild(notification);
    }
    
    // Set message and show notification
    notification.textContent = message;
    notification.style.opacity = '1';
    
    // Hide after 3 seconds
    setTimeout(() => {
      notification.style.opacity = '0';
    }, 3000);
  }
  
  /**
   * Process gaze data from WebGazer
   * @param {Object} data - Gaze data from WebGazer
   */
  function processGazeData(data) {
    // Reset blink detection flag
    gazeTracking.blinkDetected = false;
    
    // Get x, y coordinates
    const x = data.x;
    const y = data.y;
    
    // Check data validity
    if (isNaN(x) || isNaN(y) || x < 0 || y < 0 || x > window.innerWidth || y > window.innerHeight) {
      handleGazeDataMissing();
      return;
    }
    
    // Store gaze data
    gazeTracking.gazeData = { x, y, timestamp: Date.now() };
    gazeTracking.gazeDataValid = true;
    
    // Add to history (keep last 30 points)
    gazeTracking.gazeHistory.push({ x, y, timestamp: Date.now() });
    if (gazeTracking.gazeHistory.length > 30) {
      gazeTracking.gazeHistory.shift();
    }
    
    // Check if looking at screen
    const inScreen = 
      x >= 0 && x <= window.innerWidth && 
      y >= 0 && y <= window.innerHeight;
    
    // Check if looking at content area
    const inContent = 
      x >= gazeTracking.contentArea.left && 
      x <= gazeTracking.contentArea.right && 
      y >= gazeTracking.contentArea.top && 
      y <= gazeTracking.contentArea.bottom;
    
    // Update status
    gazeTracking.lookingAtScreen = inScreen;
    gazeTracking.lookingAtContent = inContent;
    
    // Calculate attention percentages
    updateAttentionPercentages();
  }
  
  /**
   * Handle missing gaze data (potential blink or looking away)
   */
  function handleGazeDataMissing() {
    const now = Date.now();
    
    // If we had valid data before and now it's missing, might be a blink
    if (gazeTracking.gazeDataValid) {
      const timeSinceLastValidData = now - gazeTracking.gazeData.timestamp;
      
      // If data missing for less than 400ms, likely a blink
      if (timeSinceLastValidData < 400) {
        detectBlink();
      } 
      // If data missing for longer, likely looking away
      else if (timeSinceLastValidData > 500) {
        gazeTracking.lookingAtScreen = false;
        gazeTracking.lookingAtContent = false;
      }
    }
    
    gazeTracking.gazeDataValid = false;
  }
  
  /**
   * Detect a blink event
   */
  function detectBlink() {
    const now = Date.now();
    
    // Don't register blinks too close together (debounce)
    if (now - gazeTracking.lastBlinkTime < 500) {
      return;
    }
    
    // Record the blink
    gazeTracking.blinkDetected = true;
    gazeTracking.blinkCount++;
    gazeTracking.lastBlinkTime = now;
    
    // Store for blink rate calculation
    gazeTracking.blinkHistory.push(now);
    
    // Keep only last minute of blinks
    while (gazeTracking.blinkHistory.length > 0 && 
           gazeTracking.blinkHistory[0] < now - 60000) {
      gazeTracking.blinkHistory.shift();
    }
    
    // Update UI with blink count
    blinkCounterElement.textContent = gazeTracking.blinkCount;
    lastBlinkTimeElement.textContent = new Date(gazeTracking.lastBlinkTime).toLocaleTimeString();
    
    // Update blink metric
    blinkMetric.textContent = 'Blinked ✓';
    setTimeout(() => {
      if (blinkMetric.textContent === 'Blinked ✓') {
        blinkMetric.textContent = 'Eyes open ✓';
      }
    }, 500);
  }
  
  /**
   * Update percentage of time looking at screen/content
   */
  function updateAttentionPercentages() {
    const now = Date.now();
    const historyDuration = 5000; // Last 5 seconds
    const cutoffTime = now - historyDuration;
    
    // Filter recent history
    const recentHistory = gazeTracking.gazeHistory.filter(item => item.timestamp > cutoffTime);
    
    if (recentHistory.length === 0) {
      return;
    }
    
    // Count points in screen and content
    const inScreenCount = recentHistory.filter(item => 
      item.x >= 0 && item.x <= window.innerWidth && 
      item.y >= 0 && item.y <= window.innerHeight
    ).length;
    
    const inContentCount = recentHistory.filter(item => 
      item.x >= gazeTracking.contentArea.left && 
      item.x <= gazeTracking.contentArea.right && 
      item.y >= gazeTracking.contentArea.top && 
      item.y <= gazeTracking.contentArea.bottom
    ).length;
    
    // Calculate percentages
    gazeTracking.inScreenPercentage = (inScreenCount / recentHistory.length) * 100;
    gazeTracking.inContentPercentage = (inContentCount / recentHistory.length) * 100;
    
    // Calculate overall attention score
    gazeTracking.attentionScore = gazeTracking.inContentPercentage / 100;
  }
  
  // =========================================================
  // 4. INTEGRATION WITH EXISTING METRICS
  // =========================================================
  
  /**
   * Get the current blink rate from WebGazer
   * @returns {number} Blinks per minute
   */
  function getGazeBlinkRate() {
    const now = Date.now();
    const oneMinuteAgo = now - 60000;
    
    // Count blinks in the last minute
    const blinkCount = gazeTracking.blinkHistory.filter(timestamp => timestamp > oneMinuteAgo).length;
    
    return blinkCount;
  }
  
  /**
   * Get the current attention level from gaze data
   * @returns {number} Attention score from 0 to 1
   */
  function getGazeAttentionScore() {
    return gazeTracking.attentionScore;
  }
  
  /**
   * Integrate gaze data with existing client metrics
   * This should be called before sending metrics to the server
   * @param {Object} clientMetrics - The metrics object to enhance
   * @returns {Object} - Enhanced metrics with gaze data
   */
  function enhanceMetricsWithGazeData(clientMetrics) {
    // Only enhance if gaze tracking is active and calibrated
    if (!gazeTracking.isTracking || !gazeTracking.isCalibrated) {
      return clientMetrics;
    }
    
    // Get current gaze metrics
    const gazeBlinks = getGazeBlinkRate();
    const gazeAttention = getGazeAttentionScore();
    
    // Add gaze-specific metrics
    const enhancedMetrics = {
      ...clientMetrics,
      gaze_metrics: {
        blink_rate: gazeBlinks,
        attention_score: gazeAttention,
        looking_at_content_percentage: gazeTracking.inContentPercentage,
        looking_at_screen_percentage: gazeTracking.inScreenPercentage
      }
    };
    
    // Use gaze-based attention score if available
    if (gazeAttention !== null) {
      enhancedMetrics.client_estimated_attention = 
        (enhancedMetrics.client_estimated_attention * 0.3) + (gazeAttention * 0.7);
    }
    
    // Use gaze-based blink detection if available
    if (gazeBlinks > 0) {
      enhancedMetrics.blink_metrics.blink_rate = gazeBlinks;
    }
    
    return enhancedMetrics;
  }
  
  // =========================================================
  // 5. INTEGRATION WITH EXISTING TRACKING SYSTEM
  // =========================================================
  
  /**
   * Updates the UI to show gaze tracking status
   * Extends the original updateTrackingStatus function
   * @param {string} mode - Tracking mode ('gaze', 'face', 'motion', 'memory', 'inactive')
   */
  function updateTrackingStatusWithGaze(mode) {
    currentTrackingMode = mode;
    trackingModeElement.textContent = mode;
    
    // Update status indicator
    trackingStatus.className = 'tracking-status';
    
    switch(mode) {
      case 'gaze':
        trackingStatus.textContent = 'Gaze Tracking';
        trackingStatus.classList.add('status-active');
        break;
      case 'face':
        trackingStatus.textContent = 'Face Tracking';
        trackingStatus.classList.add('status-active');
        break;
      case 'motion':
        trackingStatus.textContent = 'Motion Tracking';
        trackingStatus.classList.add('status-fallback');
        break;
      case 'memory':
        trackingStatus.textContent = 'Memory Tracking';
        trackingStatus.classList.add('status-fallback');
        break;
      case 'inactive':
        trackingStatus.textContent = 'Inactive';
        trackingStatus.classList.add('status-inactive');
        break;
      default:
        trackingStatus.textContent = mode;
        trackingStatus.classList.add('status-inactive');
    }
  }
  
  /**
   * Enhance the existing processFrame function with gaze tracking
   * This should replace or be integrated into your existing processFrame function
   * 
   * Note: Call this instead of the original processFrame function after initializing WebGazer
   */
  async function enhancedProcessFrame() {
    if (isPaused) {
      // If paused, just request next frame without processing
      requestAnimationFrame(enhancedProcessFrame);
      return;
    }
    
    // Calculate FPS (existing code)
    const now = Date.now();
    const elapsed = now - lastFrameTime;
    lastFrameTime = now;
    
    faceDetectionTimes.push(elapsed);
    if (faceDetectionTimes.length > 30) {
      faceDetectionTimes.shift();
    }
    
    const avgTime = faceDetectionTimes.reduce((sum, time) => sum + time, 0) / faceDetectionTimes.length;
    const fps = Math.round(1000 / avgTime);
    faceDetectionFPSElement.textContent = fps;
    
    // Clear canvas contexts
    const ctx = canvas.getContext('2d');
    ctx.clearRect(0, 0, canvas.width, canvas.height);
    
    // First check if we have valid gaze data
    if (gazeTracking.isTracking && gazeTracking.isCalibrated && gazeTracking.gazeDataValid) {
      // Use WebGazer as primary tracking method
      updateTrackingStatus('gaze');
      
      // Update metrics based on gaze data
      if (gazeTracking.blinkDetected) {
        // Reset to prevent double counting
        gazeTracking.blinkDetected = false;
        
        // Blink metrics already updated in the detectBlink function
      } else {
        // No blink detected - update with open eyes status
        blinkMetric.textContent = 'Eyes open ✓';
      }
      
      // Update focus metrics based on gaze
      if (gazeTracking.lookingAtContent) {
        focusMetric.textContent = 'Looking at content ✓';
        headPoseMetric.textContent = 'Attentive ✓';
      } else if (gazeTracking.lookingAtScreen) {
        focusMetric.textContent = 'Looking away from content ⚠️';
        headPoseMetric.textContent = 'Distracted ⚠️';
      } else {
        focusMetric.textContent = 'Looking away from screen ⚠️';
        headPoseMetric.textContent = 'Not visible ⚠️';
      }
      
      // Activity metric
      activityMetric.textContent = gazeTracking.lookingAtScreen ? 'Active ✓' : 'Inactive ⚠️';
      
    } else {
      // Fall back to face-api.js tracking if WebGazer not available
      try {
        // Check if faceapi is available
        if (typeof faceapi === 'undefined') {
          // Face API not available, run in motion-only mode
          updateTrackingStatus('motion');
          motionLevel = detectMotion(video);
          motionLevelElement.textContent = Math.round(motionLevel);
          motionDetected = motionLevel > settings.motionSensitivity;
          
          // Update metrics for motion-only mode
          blinkMetric.textContent = 'Face API not available';
          focusMetric.textContent = 'Using motion tracking only';
          headPoseMetric.textContent = 'Face tracking not available';
          activityMetric.textContent = motionDetected ? 'Movement detected ✓' : 'No activity detected ⚠️';
          
          // Continue processing frames
          requestAnimationFrame(enhancedProcessFrame);
          return;
        }
        
        // Try to detect face using face-api.js (existing code)
        const options = new faceapi.TinyFaceDetectorOptions({ 
          inputSize: 160, 
          scoreThreshold: settings.faceDetectionConfidence
        });
        
        const result = await faceapi.detectSingleFace(video, options).withFaceLandmarks();
        
        // Detect motion (runs regardless of face detection)
        motionLevel = detectMotion(video);
        motionLevelElement.textContent = Math.round(motionLevel);
        
        // Check motion threshold
        motionDetected = motionLevel > settings.motionSensitivity;
        
        // Handle face detection result
        if (result) {
          // Face found - continue with existing face-api.js processing
          // ...existing face detection code...
          
          // Face found! Update last detection time and data
          lastDetectedFace = result;
          lastFaceDetectionTime = now;
          
          // Switch to face tracking mode
          updateTrackingStatus('face');
          
          // Draw facial landmarks if enabled
          if (settings.showFaceMesh) {
            faceapi.draw.drawFaceLandmarks(canvas, result);
          }
          
          // Process facial features
          const landmarks = result.landmarks;
          const leftEye = landmarks.getLeftEye();
          const rightEye = landmarks.getRightEye();
          
          // Calculate eye aspect ratio for both eyes
          const leftEAR = calculateEyeAspectRatio(leftEye);
          const rightEAR = calculateEyeAspectRatio(rightEye);
          
          // Average of both eyes
          const earValue = (leftEAR + rightEAR) / 2;
          eyeAspectRatioElement.textContent = earValue.toFixed(3);
          
          // Detect blink (eye closure followed by opening)
          const previousEyesClosed = eyesClosed;
          eyesClosed = earValue < settings.eyeAspectRatioThreshold;
          
          // Process blink detection
          if (previousEyesClosed && !eyesClosed) {
            blinkCount++;
            lastBlinkTime = now;
            blinkCounterElement.textContent = blinkCount;
            lastBlinkTimeElement.textContent = new Date(lastBlinkTime).toLocaleTimeString();
            
            // Update UI
            blinkMetric.textContent = 'Blinked ✓';
            setTimeout(() => {
              if (blinkMetric.textContent === 'Blinked ✓') {
                blinkMetric.textContent = 'Eyes open ✓';
              }
            }, 500);
          } else if (eyesClosed) {
            blinkMetric.textContent = 'Eyes closed 👀';
          } else {
            blinkMetric.textContent = 'Eyes open ✓';
          }
          
          // Calculate blink rate per minute (existing code)
          const oneMinuteAgo = now - 60000;
          
          // Store timestamped blinks for rate calculation
          if (previousEyesClosed && !eyesClosed) {
            blinkRates.push(now);
          }
          
          // Remove blinks older than one minute
          blinkRates = blinkRates.filter(time => time > oneMinuteAgo);
          
          // Calculate blinks per minute
          const blinksPerMinute = blinkRates.length;
          blinkRateValue.textContent = blinksPerMinute;
          
          // Determine focus based on blink rate
          if (blinksPerMinute < 5) {
            focusMetric.textContent = 'Potential eye strain ⚠️';
          } else if (blinksPerMinute > 30) {
            focusMetric.textContent = 'Potential fatigue ⚠️';
          } else {
            focusMetric.textContent = 'Normal blink rate ✓';
          }
          
          // Calculate head orientation
          calculateHeadOrientation(landmarks);
          
          // Hide any error message since detection is working
          hideError();
        } else {
          // No face detected - handle as in original code
          // ...existing fallback code...
          
          // No face detected. Check if we're within memory duration
          const timeSinceLastDetection = now - lastFaceDetectionTime;
          const faceMemoryMs = settings.faceMemoryDuration * 1000;
          
          if (lastDetectedFace && timeSinceLastDetection < faceMemoryMs) {
            // We still remember the face - use last known position
            if (settings.showFaceMesh) {
              // Draw faded face landmarks from memory
              ctx.globalAlpha = 0.3;
              faceapi.draw.drawFaceLandmarks(canvas, lastDetectedFace);
              ctx.globalAlpha = 1.0;
            }
            
            // Check if there's motion or input activity
            if (motionDetected || (now - lastInputEventTime < 5000)) {
              // Motion detected - assume user is still present but face isn't visible
              updateTrackingStatus('motion');
              
              // Update metrics
              blinkMetric.textContent = 'Face not visible, using motion';
              activityMetric.textContent = 'Movement detected ✓';
            } else {
              // No motion detected, but still within face memory window
              updateTrackingStatus('memory');
              activityMetric.textContent = 'Low activity ⚠️';
            }
          } else {
            // No face detected for too long and no motion - tracking lost
            updateTrackingStatus('inactive');
            blinkMetric.textContent = 'No face detected';
            focusMetric.textContent = 'Face not detected';
            headPoseMetric.textContent = 'Face not detected';
            activityMetric.textContent = 'No activity detected ⚠️';
          }
        }
      } catch (err) {
        console.error('Error in frame processing:', err);
        showError(`Face detection error: ${err.message}`);
        updateTrackingStatus('error');
      }
    }
    
    // Send metrics to server at regular intervals when in a session
    if (currentSessionId && (now % 5000 < 200)) {
      sendEnhancedClientMetrics();
    }
    
    // Continue processing frames
    requestAnimationFrame(enhancedProcessFrame);
  }
  
  /**
   * Send enhanced metrics to the server
   * Replaces the original sendClientMetrics function
   */
  async function sendEnhancedClientMetrics() {
    if (!currentSessionId || isPaused) return;
    
    try {
      // Calculate original metrics
      const now = Date.now();
      const oneMinuteAgo = now - 60000;
      const recentBlinks = blinkRates.filter(time => time > oneMinuteAgo);
      const blinksPerMinute = recentBlinks.length;
      
      // Calculate average time between blinks
      let avgBlinkDuration = 0;
      if (recentBlinks.length > 1) {
        let totalDuration = 0;
        for (let i = 1; i < recentBlinks.length; i++) {
          totalDuration += recentBlinks[i] - recentBlinks[i-1];
        }
        avgBlinkDuration = totalDuration / (recentBlinks.length - 1);
      }
      
      // Original attention estimate
      let attentionEstimate = 1.0;
      
      // Reduce attention if currently not using face tracking
      if (currentTrackingMode !== 'face' && currentTrackingMode !== 'gaze') {
        attentionEstimate *= 0.8;
      }
      
      // If blink rate is too low, attention might be hyperfocused or straining
      if (blinksPerMinute < 5) {
        attentionEstimate *= 0.9;
      }
      // If blink rate is too high, attention might be decreasing
      else if (blinksPerMinute > 30) {
        attentionEstimate *= 0.8;
      }
      
      // Factor in input events (keyboard/mouse activity)
      const inputActive = (now - lastInputEventTime < 30000);
      if (inputActive) {
        // Some input activity can indicate engagement
        attentionEstimate = Math.min(1.0, attentionEstimate + 0.1);
      }
      
      // Create base client metrics
      const clientMetrics = {
        timestamp: new Date().toISOString(),
        blink_metrics: {
          blink_count: blinkCount,
          blink_rate: blinksPerMinute,
          avg_blink_duration: avgBlinkDuration
        },
        seconds_since_last_movement: Math.round((now - Math.max(lastBlinkTime, lastInputEventTime)) / 1000),
        client_estimated_attention: attentionEstimate,
        tracking_mode: currentTrackingMode,
        motion_level: motionLevel,
        input_events: inputEventCount
      };
      
      // Enhance with gaze tracking data if available
      const enhancedMetrics = enhanceMetricsWithGazeData(clientMetrics);
      
      console.log("Sending enhanced client metrics:", enhancedMetrics);
      
      const response = await fetch(`${API_URL}/submit-client-metrics/${currentSessionId}`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json'
        },
        body: JSON.stringify(enhancedMetrics)
      });
      
      const data = await response.json();
      
      // Display suggestion if provided
      if (data.suggestion) {
        suggestionDiv.textContent = data.suggestion;
        suggestionDiv.style.display = 'block';
        
        // Add suggestion to insights
        addInsight(data.suggestion);
      }
      
    } catch (err) {
      console.error('Error sending client metrics:', err);
      showError(`Error sending metrics to server: ${err.message}`);
    }
  }